from rouge_score import rouge_scorer, scoring
from sacrebleu import corpus_bleu
from evaluate import load
from typing import Dict, List
from tqdm import tqdm
import numpy as np
import re

try:
    import nltk

    NLTK_AVAILABLE = True
except (ImportError, ModuleNotFoundError):
    NLTK_AVAILABLE = False

if NLTK_AVAILABLE:
    try:
        nltk.download("punkt", quiet=True)
    except FileExistsError:  # multiprocessing race condition
        pass

def add_newline_to_end_of_each_sentence(x: str) -> str:
    re.sub("<n>", "", x)  # remove pegasus newline char
    assert NLTK_AVAILABLE, "nltk must be installed to separate newlines betwee sentences. (pip install nltk)"
    return "\n".join(nltk.sent_tokenize(x))

def calcualte_bertscore(output_lns, refs_lns, **kwarg) -> dict:
    print("start bertscore evaluation")
    results = {'precision':[], 'recall':[], 'f1':[], 'hashcode':None}
    bertscore = load("bertscore")
    for output, ref in tqdm(zip(output_lns, refs_lns), total=len(output_lns)):
        result = bertscore.compute(predictions=[output], references=[ref], lang="en")
        results['precision'] += result['precision']
        results['recall'] += result['recall']
        results['f1'] += result['f1']
        results['hashcode'] = result['hashcode']
    results['precision'] = sum(results['precision']) / len(results['precision'])
    results['recall'] = sum(results['recall']) / len(results['recall'])
    results['f1'] = sum(results['f1']) / len(results['f1'])
    return results

def calculate_bleu(output_lns, refs_lns, **kwargs) -> dict:
    """Uses sacrebleu's corpus_bleu implementation."""
    return {"bleu": corpus_bleu(output_lns, [refs_lns], **kwargs).score}

def extract_rouge_mid_statistics(dct):
    new_dict = {}
    for k1, v1 in dct.items():
        mid = v1.mid
        new_dict[k1] = {stat: round(getattr(mid, stat), 4) for stat in ["precision", "recall", "fmeasure"]}
    return new_dict

ROUGE_KEYS = ["rouge1", "rouge2", "rougeL", "rougeLsum"]

def calculate_rouge(
    pred_lns: List[str],
    tgt_lns: List[str],
    use_stemmer=True,
    rouge_keys=["rouge1", "rouge2","rougeLsum"],
    return_precision_and_recall=True,
    bootstrap_aggregation=True,
    newline_sep=True,
) -> Dict:
    """Calculate rouge using rouge_scorer package.
    Args:
        pred_lns: list of summaries generated by model
        tgt_lns: list of groundtruth summaries (e.g. contents of val.target)
        use_stemmer:  Bool indicating whether Porter stemmer should be used to
        strip word suffixes to improve matching.
        rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum
        return_precision_and_recall: (False) whether to also return precision and recall.
        bootstrap_aggregation: whether to do the typical bootstrap resampling of scores. Defaults to True, if False
            this function returns a collections.defaultdict[metric: list of values for each observation for each subscore]``
        newline_sep:(default=True) whether to add newline between sentences. This is essential for calculation rougeL
        on multi sentence summaries (CNN/DM dataset).
    Returns:
         Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys
    """
    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)
    aggregator = scoring.BootstrapAggregator()
    print("start rouge evaluation")
    i = 0
    for pred, tgt in zip(tgt_lns, pred_lns):
        # rougeLsum expects "\n" separated sentences within a summary
        if newline_sep:
            pred = add_newline_to_end_of_each_sentence(pred)
            tgt = add_newline_to_end_of_each_sentence(tgt)
        scores = scorer.score(pred, tgt)
        aggregator.add_scores(scores)
    
    print("start rouge aggregation")
    if bootstrap_aggregation:
        result = aggregator.aggregate()
        if return_precision_and_recall:
            return extract_rouge_mid_statistics(result)  # here we return dict
        else:
            return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}

    else:
        return aggregator._scores  # here we return defaultdict(list)

def evaluate(output_lns: List[str],
             reference_lns: List[str],
             score_fn = calculate_rouge,
             seed : int = 1234) -> dict:

    np.random.seed(seed)
    scores: dict = score_fn(output_lns, reference_lns)
    return scores

if __name__ == "__main__":
    reference = ['The dog bit the man.', 'It was not unexpected.','The man bit him first.']
    pred = ['The dog bit the man.', "It wasn't surprising.", 'The man had just bitten him.']

    scores = evaluate(pred, reference, score_fn=calculate_rouge)
    print(scores)